{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"General A Redis Cluster Client A Redis Cluster Client with a cli line tools in the spirit of redis-trib. Installation UNIX one liner /bin/sh -c \"`curl -fsSL https://raw.githubusercontent.com/machinezone/rcc/master/tools/install.sh`\" You can see what the install script is doing first here . For folks familiar with Python cd $HOME # or anywhere you want python3 -mvenv venv source venv/bin/activate pip install rcc rcc Rationale rcc started as an attempt at writing an asyncio redis-cluster aware python client. It is now mostly used a redis cluster tool, built on a minimal but functional redis client library. The main asyncio redis library, aioredis does not support redis cluster at this point. There is another library named aredis which has cluster support, but which has some small bugs for which pull requests existed, that were not merged until recently. Getting a redis client to work is not terribly hard, thanks to the design of redis, so I started this project and got it to work in a limited amount of time. Tools Several tools come with this package, as subcommands of the main cli named rcc . Some documentation (wip) is available here . keyspace / will turn on redis keyspace notifications and tell you what your hot keys are. binpacking / will help reshard your cluster in an optimal way based on your usage. This will consume the output of the keyspace command Contributing rcc is developed on GitHub . We'd love to hear about how you use it; opening up an issue on GitHub is ok for that. If things don't work as expected, please create an issue on GitHub, or even better a pull request if you know how to fix your problem.","title":"General"},{"location":"#general","text":"A Redis Cluster Client A Redis Cluster Client with a cli line tools in the spirit of redis-trib.","title":"General"},{"location":"#installation","text":"","title":"Installation"},{"location":"#unix-one-liner","text":"/bin/sh -c \"`curl -fsSL https://raw.githubusercontent.com/machinezone/rcc/master/tools/install.sh`\" You can see what the install script is doing first here .","title":"UNIX one liner"},{"location":"#for-folks-familiar-with-python","text":"cd $HOME # or anywhere you want python3 -mvenv venv source venv/bin/activate pip install rcc rcc","title":"For folks familiar with Python"},{"location":"#rationale","text":"rcc started as an attempt at writing an asyncio redis-cluster aware python client. It is now mostly used a redis cluster tool, built on a minimal but functional redis client library. The main asyncio redis library, aioredis does not support redis cluster at this point. There is another library named aredis which has cluster support, but which has some small bugs for which pull requests existed, that were not merged until recently. Getting a redis client to work is not terribly hard, thanks to the design of redis, so I started this project and got it to work in a limited amount of time.","title":"Rationale"},{"location":"#tools","text":"Several tools come with this package, as subcommands of the main cli named rcc . Some documentation (wip) is available here . keyspace / will turn on redis keyspace notifications and tell you what your hot keys are. binpacking / will help reshard your cluster in an optimal way based on your usage. This will consume the output of the keyspace command","title":"Tools"},{"location":"#contributing","text":"rcc is developed on GitHub . We'd love to hear about how you use it; opening up an issue on GitHub is ok for that. If things don't work as expected, please create an issue on GitHub, or even better a pull request if you know how to fix your problem.","title":"Contributing"},{"location":"CHANGELOG/","text":"Changelog All changes to this project will be documented in this file. [0.0.5] - 2020-02-04 missing sys import [0.0.4] - 2020-02-04 remove debug printf [0.0.2] - 2020-02-03 implement DEL command + add a very simple cli like redis-cli","title":"Changelog"},{"location":"CHANGELOG/#changelog","text":"All changes to this project will be documented in this file.","title":"Changelog"},{"location":"CHANGELOG/#005-2020-02-04","text":"missing sys import","title":"[0.0.5] - 2020-02-04"},{"location":"CHANGELOG/#004-2020-02-04","text":"remove debug printf","title":"[0.0.4] - 2020-02-04"},{"location":"CHANGELOG/#002-2020-02-03","text":"implement DEL command + add a very simple cli like redis-cli","title":"[0.0.2] - 2020-02-03"},{"location":"binpacking/","text":"Binpacking Bin Packing The bin packing problem is about optimizing the distribution of weighted items to bins. Multiple variants exist, but our focus in this post is on distributing weights into a fixed number of bins. I like to think about Santa Claus getting ready to deliver presents the day before Christmas, and having his elves preparing the reindeers' carriages. We will imagine that many carriages are ready to be used, and that Santa will jump to a new one once he has delivered all the presents in his carriage. The question is: Depending on the size of the presents boxes, how should we distribute the load so that each carriage is as evenly distributed as possible ? Luckily a Python library exists for this algorithm (as always?) and the library interface is very simple and clear, as it uses Python dictionaries. # We have 6 presents b = { 'a': 10, 'b': 10, 'c':11, 'd':1, 'e': 2,'f':7 } # [name, weight] pairs, in a dictionary # We want to pack them into 4 bins # Assign each pair to its own bin, and return the list of bins as a result bins = binpacking.to_constant_bin_number(b, 4) # Print the result. Excuse the ugly uppercase Bin, but bin is a reserved function in Python for Bin in bins: # We will print the sum of all the weights in the bin, along with the bin itself print(sum(Bin.values()), Bin) Executing this program prints this on the console: 11 {'c': 11} 10 {'b': 10} 10 {'a': 10} 10 {'f': 7, 'e': 2, 'd': 1} We have 4 bins with a weight of 10 or 11, so things are pretty well distributed. Now imagine that we were cycling through each objects in our original list of items b, and picking the destination bin randomly . The following program does that. bins = [{} for _ in range(4)] # Create 4 bins, each bin is a list of dictionaries for name, weight in b.items(): binIdx = random.randint(0,3) # pick a random integer between 0 and 3 bins[binIdx][name] = weight # insert a 'present' into a bin Let's run this 3 times. # First try 9 {'e': 2, 'f': 7} 1 {'d': 1} 31 {'a': 10, 'b': 10, 'c': 11} 0 {} # Second try 12 {'a': 10, 'e': 2} 18 {'c': 11, 'f': 7} 11 {'b': 10, 'd': 1} 0 {} # Third try 0 {} 22 {'b': 10, 'c': 11, 'd': 1} 2 {'e': 2} 17 {'a': 10, 'f': 7} This is pretty terrible; however, this is what randomly picking an item will give you. Redis Cluster To quote its documentation, Redis Cluster provides a way to run a Redis installation where data is automatically sharded across multiple Redis nodes . Redis cluster isn't only about sharding, but this is what we will focus on. Every time you store something in Redis Cluster, it all starts by hashing the key of your object with a CRC16, modulo something, and then assigning a hash value to a slot, called a hash slot . There are 16384 hash slots in Redis Cluster. A good hash function should map the expected inputs as evenly as possible over its output range; that is 16384 hash slots in our case. That property is called uniformity . Each node in a cluster is responsible for a range of hash slots, and that assignment is usually done automatically early on, but the power of this system is that the assignment can be manual as well. If you know of a great way to distribute your hash slots to nodes, you can arrange for an optimal load balancing, and keep each Redis node as busy as possible. Just like you wouldn't want to have some reindeer do more work than others, you would not want some Redis machine have little work to do, while others are crazy busy . Cobra Cobra is a real-time messaging system, which we mainly use at Machine Zone for analytics. It uses Redis internally, and store metrics events as JSON blobs into redis streams. Cobra internally keeps statistics about all channels, which helps us know which ones are high frequency. This is displayed in the table below, along the Redis hash slot for a given channel name. Notice that the CRC16 assigns unique hash values for all our sample keys. We can also see that the distribution of published items by channel name is uneven. If we were to randomly pick a channel and have it processed by a random Redis node, that would lead to inefficiencies . Channel name Published items Redis Hash Slot ------------ --------------- --------------- channel_1 16 1732 channel_2 46889 6454 channel_3 4284 4050 channel_4 13444 803 channel_5 46745 754 channel_6 14714 3791 channel_7 4251 4152 channel_8 4356 14143 channel_9 677400 6417 channel_10 4322 7208 channel_11 163277 11316 channel_12 5585 7132 channel_13 360998 2476 channel_14 485541 14863 channel_15 3811 9883 channel_16 2 4959 channel_17 4339 6884 channel_18 4392 15400 channel_19 9307 6629 channel_20 685 3247 channel_21 176710 15535 channel_22 264 16311 channel_23 43157 687 channel_24 2410 6831 channel_25 103027 15422 channel_26 8701 2221 channel_27 518 7627 channel_28 46 7092 channel_29 7 3113 Let's try to do the channel to hash slot assignment randomly, as it would be done if we were not assigning hash slots to Redis nodes manually. Random distribution 1 537399 {'channel_14': 485541, 'channel_23': 43157, 'channel_26': 8701} 667035 {'channel_1': 16, 'channel_3': 4284, 'channel_6': 14714, 'channel_8': 4356, 'channel_13': 360998, 'channel_16': 2, 'channel_21': 176710, 'channel_24': 2410, 'channel_25': 103027, 'channel_27': 518} 59884 {'channel_5': 46745, 'channel_7': 4251, 'channel_15': 3811, 'channel_17': 4339, 'channel_20': 685, 'channel_28': 46, 'channel_29': 7} 924880 {'channel_2': 46889, 'channel_4': 13444, 'channel_9': 677400, 'channel_10': 4322, 'channel_11': 163277, 'channel_12': 5585, 'channel_18': 4392, 'channel_19': 9307, 'channel_22': 264} stdev 376758.0808394682 We will represent this visually, using a pie chart. The excellent matplotlib library can plot this for us. labels = ['1', '2', '3', '4'] S = sum(weights) sizes = [100 * weight / S for weight in weights] colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral'] patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=90) plt.legend(patches, labels, loc=\"best\") plt.axis('equal') plt.tight_layout() plt.show() This isn't a great distribution. Node 3 got very few hash slots assigned, while Node 4 seems to do too much work. Random distribution 2 821388 {'channel_7': 4251, 'channel_11': 163277, 'channel_12': 5585, 'channel_13': 360998, 'channel_18': 4392, 'channel_20': 685, 'channel_21': 176710, 'channel_24': 2410, 'channel_25': 103027, 'channel_28': 46, 'channel_29': 7} 75311 {'channel_6': 14714, 'channel_10': 4322, 'channel_15': 3811, 'channel_19': 9307, 'channel_23': 43157} 1274894 {'channel_1': 16, 'channel_2': 46889, 'channel_4': 13444, 'channel_5': 46745, 'channel_9': 677400, 'channel_14': 485541, 'channel_16': 2, 'channel_17': 4339, 'channel_27': 518} 17605 {'channel_3': 4284, 'channel_8': 4356, 'channel_22': 264, 'channel_26': 8701} stdev 493807.17986663507 This one is even worse; Node 3 got assigned a lot of slots, pretty bad. Ideally we would want to see a pie chart with four somewhat equal quadrants. However, if we can find in which hash slot each channel traffic will land, and use the published count as a weight for that channel, we can allocate that channel to be handled by a Redis Cluster node using the Bin Packing algorithm . This will give us a more fair or optimal load balancing. 2 Redis commands are required to do that: The CLUSTER KEYSLOT command run the CRC16 hash on a key. The CLUSTER SETSLOT command will be used to allocate a hash slot to a Redis cluster node. Practically speaking, it is easier to use the redis-cli command which can do that for us, and handle the resharding better. Here are all the things that the CLI can do for you. $ redis-cli --cluster help Cluster Manager Commands: create host1:port1 ... hostN:portN --cluster-replicas <arg> check host:port --cluster-search-multiple-owners info host:port fix host:port --cluster-search-multiple-owners reshard host:port --cluster-from <arg> --cluster-to <arg> --cluster-slots <arg> --cluster-yes --cluster-timeout <arg> --cluster-pipeline <arg> --cluster-replace rebalance host:port --cluster-weight <node1=w1...nodeN=wN> --cluster-use-empty-masters --cluster-timeout <arg> --cluster-simulate --cluster-pipeline <arg> --cluster-threshold <arg> --cluster-replace add-node new_host:new_port existing_host:existing_port --cluster-slave --cluster-master-id <arg> del-node host:port node_id call host:port command arg arg .. arg set-timeout host:port milliseconds import host:port --cluster-from <arg> --cluster-copy --cluster-replace help Back to the bin-packing algorithm, let's see how it performs on our own real data set, which is a less contrived than the one we used in our first example. Bin packing distribution 677400 {'channel_9': 677400} 503781 {'channel_14': 485541, 'channel_26': 8701, 'channel_17': 4339, 'channel_7': 4251, 'channel_20': 685, 'channel_22': 264} 503736 {'channel_13': 360998, 'channel_2': 46889, 'channel_5': 46745, 'channel_6': 14714, 'channel_4': 13444, 'channel_19': 9307, 'channel_8': 4356, 'channel_3': 4284, 'channel_24': 2410, 'channel_27': 518, 'channel_28': 46, 'channel_1': 16, 'channel_29': 7, 'channel_16': 2} 504281 {'channel_21': 176710, 'channel_11': 163277, 'channel_25': 103027, 'channel_23': 43157, 'channel_12': 5585, 'channel_18': 4392, 'channel_10': 4322, 'channel_15': 3811} Our 4 bins receive around 500,000 units of weight each. If we visualize this with a pie chart, this feels like a pretty fair repartition. Conclusion If you can efficiently analyze your key space , the flexible Redis Cluster hash slot design lets you do wonders and achieve an excellent load balancing . This leads to less CPU cycles burnt for nothing, which leads to melting the ice cap perhaps a bit more slowly, which in turn lets Santa live the life to which he is accustomed to in the North Pole for the foreseeable future.","title":"Binpacking"},{"location":"binpacking/#binpacking","text":"","title":"Binpacking"},{"location":"binpacking/#bin-packing","text":"The bin packing problem is about optimizing the distribution of weighted items to bins. Multiple variants exist, but our focus in this post is on distributing weights into a fixed number of bins. I like to think about Santa Claus getting ready to deliver presents the day before Christmas, and having his elves preparing the reindeers' carriages. We will imagine that many carriages are ready to be used, and that Santa will jump to a new one once he has delivered all the presents in his carriage. The question is: Depending on the size of the presents boxes, how should we distribute the load so that each carriage is as evenly distributed as possible ? Luckily a Python library exists for this algorithm (as always?) and the library interface is very simple and clear, as it uses Python dictionaries. # We have 6 presents b = { 'a': 10, 'b': 10, 'c':11, 'd':1, 'e': 2,'f':7 } # [name, weight] pairs, in a dictionary # We want to pack them into 4 bins # Assign each pair to its own bin, and return the list of bins as a result bins = binpacking.to_constant_bin_number(b, 4) # Print the result. Excuse the ugly uppercase Bin, but bin is a reserved function in Python for Bin in bins: # We will print the sum of all the weights in the bin, along with the bin itself print(sum(Bin.values()), Bin) Executing this program prints this on the console: 11 {'c': 11} 10 {'b': 10} 10 {'a': 10} 10 {'f': 7, 'e': 2, 'd': 1} We have 4 bins with a weight of 10 or 11, so things are pretty well distributed. Now imagine that we were cycling through each objects in our original list of items b, and picking the destination bin randomly . The following program does that. bins = [{} for _ in range(4)] # Create 4 bins, each bin is a list of dictionaries for name, weight in b.items(): binIdx = random.randint(0,3) # pick a random integer between 0 and 3 bins[binIdx][name] = weight # insert a 'present' into a bin Let's run this 3 times. # First try 9 {'e': 2, 'f': 7} 1 {'d': 1} 31 {'a': 10, 'b': 10, 'c': 11} 0 {} # Second try 12 {'a': 10, 'e': 2} 18 {'c': 11, 'f': 7} 11 {'b': 10, 'd': 1} 0 {} # Third try 0 {} 22 {'b': 10, 'c': 11, 'd': 1} 2 {'e': 2} 17 {'a': 10, 'f': 7} This is pretty terrible; however, this is what randomly picking an item will give you.","title":"Bin Packing"},{"location":"binpacking/#redis-cluster","text":"To quote its documentation, Redis Cluster provides a way to run a Redis installation where data is automatically sharded across multiple Redis nodes . Redis cluster isn't only about sharding, but this is what we will focus on. Every time you store something in Redis Cluster, it all starts by hashing the key of your object with a CRC16, modulo something, and then assigning a hash value to a slot, called a hash slot . There are 16384 hash slots in Redis Cluster. A good hash function should map the expected inputs as evenly as possible over its output range; that is 16384 hash slots in our case. That property is called uniformity . Each node in a cluster is responsible for a range of hash slots, and that assignment is usually done automatically early on, but the power of this system is that the assignment can be manual as well. If you know of a great way to distribute your hash slots to nodes, you can arrange for an optimal load balancing, and keep each Redis node as busy as possible. Just like you wouldn't want to have some reindeer do more work than others, you would not want some Redis machine have little work to do, while others are crazy busy .","title":"Redis Cluster"},{"location":"binpacking/#cobra","text":"Cobra is a real-time messaging system, which we mainly use at Machine Zone for analytics. It uses Redis internally, and store metrics events as JSON blobs into redis streams. Cobra internally keeps statistics about all channels, which helps us know which ones are high frequency. This is displayed in the table below, along the Redis hash slot for a given channel name. Notice that the CRC16 assigns unique hash values for all our sample keys. We can also see that the distribution of published items by channel name is uneven. If we were to randomly pick a channel and have it processed by a random Redis node, that would lead to inefficiencies . Channel name Published items Redis Hash Slot ------------ --------------- --------------- channel_1 16 1732 channel_2 46889 6454 channel_3 4284 4050 channel_4 13444 803 channel_5 46745 754 channel_6 14714 3791 channel_7 4251 4152 channel_8 4356 14143 channel_9 677400 6417 channel_10 4322 7208 channel_11 163277 11316 channel_12 5585 7132 channel_13 360998 2476 channel_14 485541 14863 channel_15 3811 9883 channel_16 2 4959 channel_17 4339 6884 channel_18 4392 15400 channel_19 9307 6629 channel_20 685 3247 channel_21 176710 15535 channel_22 264 16311 channel_23 43157 687 channel_24 2410 6831 channel_25 103027 15422 channel_26 8701 2221 channel_27 518 7627 channel_28 46 7092 channel_29 7 3113 Let's try to do the channel to hash slot assignment randomly, as it would be done if we were not assigning hash slots to Redis nodes manually.","title":"Cobra"},{"location":"binpacking/#random-distribution-1","text":"537399 {'channel_14': 485541, 'channel_23': 43157, 'channel_26': 8701} 667035 {'channel_1': 16, 'channel_3': 4284, 'channel_6': 14714, 'channel_8': 4356, 'channel_13': 360998, 'channel_16': 2, 'channel_21': 176710, 'channel_24': 2410, 'channel_25': 103027, 'channel_27': 518} 59884 {'channel_5': 46745, 'channel_7': 4251, 'channel_15': 3811, 'channel_17': 4339, 'channel_20': 685, 'channel_28': 46, 'channel_29': 7} 924880 {'channel_2': 46889, 'channel_4': 13444, 'channel_9': 677400, 'channel_10': 4322, 'channel_11': 163277, 'channel_12': 5585, 'channel_18': 4392, 'channel_19': 9307, 'channel_22': 264} stdev 376758.0808394682 We will represent this visually, using a pie chart. The excellent matplotlib library can plot this for us. labels = ['1', '2', '3', '4'] S = sum(weights) sizes = [100 * weight / S for weight in weights] colors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral'] patches, texts = plt.pie(sizes, colors=colors, shadow=True, startangle=90) plt.legend(patches, labels, loc=\"best\") plt.axis('equal') plt.tight_layout() plt.show() This isn't a great distribution. Node 3 got very few hash slots assigned, while Node 4 seems to do too much work.","title":"Random distribution 1"},{"location":"binpacking/#random-distribution-2","text":"821388 {'channel_7': 4251, 'channel_11': 163277, 'channel_12': 5585, 'channel_13': 360998, 'channel_18': 4392, 'channel_20': 685, 'channel_21': 176710, 'channel_24': 2410, 'channel_25': 103027, 'channel_28': 46, 'channel_29': 7} 75311 {'channel_6': 14714, 'channel_10': 4322, 'channel_15': 3811, 'channel_19': 9307, 'channel_23': 43157} 1274894 {'channel_1': 16, 'channel_2': 46889, 'channel_4': 13444, 'channel_5': 46745, 'channel_9': 677400, 'channel_14': 485541, 'channel_16': 2, 'channel_17': 4339, 'channel_27': 518} 17605 {'channel_3': 4284, 'channel_8': 4356, 'channel_22': 264, 'channel_26': 8701} stdev 493807.17986663507 This one is even worse; Node 3 got assigned a lot of slots, pretty bad. Ideally we would want to see a pie chart with four somewhat equal quadrants. However, if we can find in which hash slot each channel traffic will land, and use the published count as a weight for that channel, we can allocate that channel to be handled by a Redis Cluster node using the Bin Packing algorithm . This will give us a more fair or optimal load balancing. 2 Redis commands are required to do that: The CLUSTER KEYSLOT command run the CRC16 hash on a key. The CLUSTER SETSLOT command will be used to allocate a hash slot to a Redis cluster node. Practically speaking, it is easier to use the redis-cli command which can do that for us, and handle the resharding better. Here are all the things that the CLI can do for you. $ redis-cli --cluster help Cluster Manager Commands: create host1:port1 ... hostN:portN --cluster-replicas <arg> check host:port --cluster-search-multiple-owners info host:port fix host:port --cluster-search-multiple-owners reshard host:port --cluster-from <arg> --cluster-to <arg> --cluster-slots <arg> --cluster-yes --cluster-timeout <arg> --cluster-pipeline <arg> --cluster-replace rebalance host:port --cluster-weight <node1=w1...nodeN=wN> --cluster-use-empty-masters --cluster-timeout <arg> --cluster-simulate --cluster-pipeline <arg> --cluster-threshold <arg> --cluster-replace add-node new_host:new_port existing_host:existing_port --cluster-slave --cluster-master-id <arg> del-node host:port node_id call host:port command arg arg .. arg set-timeout host:port milliseconds import host:port --cluster-from <arg> --cluster-copy --cluster-replace help Back to the bin-packing algorithm, let's see how it performs on our own real data set, which is a less contrived than the one we used in our first example.","title":"Random distribution 2"},{"location":"binpacking/#bin-packing-distribution","text":"677400 {'channel_9': 677400} 503781 {'channel_14': 485541, 'channel_26': 8701, 'channel_17': 4339, 'channel_7': 4251, 'channel_20': 685, 'channel_22': 264} 503736 {'channel_13': 360998, 'channel_2': 46889, 'channel_5': 46745, 'channel_6': 14714, 'channel_4': 13444, 'channel_19': 9307, 'channel_8': 4356, 'channel_3': 4284, 'channel_24': 2410, 'channel_27': 518, 'channel_28': 46, 'channel_1': 16, 'channel_29': 7, 'channel_16': 2} 504281 {'channel_21': 176710, 'channel_11': 163277, 'channel_25': 103027, 'channel_23': 43157, 'channel_12': 5585, 'channel_18': 4392, 'channel_10': 4322, 'channel_15': 3811} Our 4 bins receive around 500,000 units of weight each. If we visualize this with a pie chart, this feels like a pretty fair repartition.","title":"Bin packing distribution"},{"location":"binpacking/#conclusion","text":"If you can efficiently analyze your key space , the flexible Redis Cluster hash slot design lets you do wonders and achieve an excellent load balancing . This leads to less CPU cycles burnt for nothing, which leads to melting the ice cap perhaps a bit more slowly, which in turn lets Santa live the life to which he is accustomed to in the North Pole for the foreseeable future.","title":"Conclusion"},{"location":"cluster_ips/","text":"Cluster Ips Problem Sometimes a redis cluster node dies, and kubernetes restarts it for you. And when this happens sometimes your pod will get a new ip address. However Redis read the cluster config from a file, nodes.conf, and that file could have the old value prior to the restart. Solution The nice way This can be fixed automatically, so that when redis start it updates its ip address in nodes.conf with the one currently assigned. A nice way to do that is explained in this guide . Essentially we use a configmap that contain the script, and use that to start redis-server. An environment variable containing the pod ip is setup with this magic, in the deployment config file. env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP The shell script regexp is from the update-node.sh. #!/bin/sh REDIS_NODES=\"/data/nodes.conf\" sed -i -e \"/myself/ s/[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}/${POD_IP}/\" ${REDIS_NODES} exec \"$@\" Don't ask me to debug this regexp :) The lame way However sometimes you just want to fix things manually, before you automate them. To do that it is as simple as editing the script with an editor, and shutting down the node with redis-cli SHUTDOWN . I figured this out thanks to the new rcc cluster-check command, while trying to reshard my cluster. ~ $ rcc Usage: rcc [OPTIONS] COMMAND [ARGS]... RCC Rcc is a Redis Cluster Client Options: --version Show the version and exit. -v, --verbose --help Show this message and exit. Commands: keyspace Subscribe to a channel rcc keyspace... cli cli tool similar to redis-cli cluster-check Similar to redis-cli --cluster check Make sure all nodes... cluster-info Monitor redis metrics rcc cluster-info --stats... cluster-init cluster-nodes Monitor redis metrics rcc cluster-nodes endpoints Print endpoints associated with a redis cluster service make-cluster migrate publish Publish to a channel reshard Reshard using the bin-packing technique sub Subscribe to a channel rcc sub --redis_url... subscribe Subscribe to a channel rcc subscribe --redis_url... ~ $ rcc --version rcc, version 0.7.4 ~ $ rcc cluster-check --redis_url redis://172.24.222.56:6379 cluster unhealthy. Re-run with -v To get details you can rerun with the verbose flag, whose alias is -v. Then it's a bit painful but you'll need to visually scan for the nodes whose signature are different. This could be because slots assignments are differents between nodes, or if slave/masters assignments differs. ~ $ rcc -v cluster-check --redis_url redis://172.24.222.56:6379 2020-02-26 01:58:53 INFO redis://172.24.222.56:6379 5f9bcb66a8e67859342aa614082f1b24 balanced True coverage True 2020-02-26 01:58:53 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave ... output trimmed for readability 2020-02-26 01:58:53 INFO redis://172.27.70.50:6379 5f9bcb66a8e67859342aa614082f1b24 balanced True coverage True 2020-02-26 01:58:53 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave 2020-02-26 01:58:53 INFO redis://172.27.98.220:6379 bad1c3a12e48191a830d21cc9f855ed7 balanced True coverage True 2020-02-26 01:58:53 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.217:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave 2020-02-26 01:58:53 INFO redis://172.28.115.173:6379 5f9bcb66a8e67859342aa614082f1b24 balanced True coverage True 2020-02-26 01:58:53 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave ... 2020-02-26 01:58:53 INFO 2 unique signatures cluster unhealthy. Re-run with -v ~ $ First we need to know which node has the broken ip, so that we can connect to it. I'll use oc get -o yaml endpoints redis-cluster - ip: 172.27.98.220 nodeName: blah-blah-blah.example.com targetRef: kind: Pod name: redis-cluster-2-sl2wf namespace: cobra-live resourceVersion: \"1568323619\" uid: 4e174e94-4143-11ea-b4fb-eeeeeeeeeeee Now I know I need to rsh to redis-cluster-2-sl2wf . Once we've done that, we'll just need some vim/insert your favorite editor magic, to use the '172.27.98.220' address for the 'myself' node (the one where we are connected). The node will be rebooted, and we can ssh to it and grep for the new ip address. root@redis-cluster-2-sl2wf:/data# grep 220 nodes.conf 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379@16379 myself,master - 0 1582064573000 22 connected 0-1637 It's the right now. Let's re-run the cluster-check command, it will happilly succeed. ~ $ ~ $ ~ $ ~ $ ~ $ rcc -v cluster-check --redis_url redis://172.24.222.56:6379 2020-02-26 02:11:44 INFO redis://172.24.222.56:6379 5f9bcb66a8e67859342aa614082f1b24 balanced True coverage True 2020-02-26 02:11:44 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave ... edited for clarity ... 2020-02-26 02:11:45 INFO redis://172.30.99.146:6379 5f9bcb66a8e67859342aa614082f1b24 balanced True coverage True 2020-02-26 02:11:45 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave 2020-02-26 02:11:45 INFO 1 unique signatures cluster ok ~ $ ~ $ Hooray ! Now we can finally reshard the cluster since it is in a good state. Resharding ~ $ ~ $ rcc reshard --redis_url redis://172.24.222.56:6379 file descriptors ulimit: 1048576 resharding can be hungry, bump it with ulimit -n if needed == b77fd799122312b936e99e7038cfd34d8a30f7f0 / 172.25.189.166:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 044ccfbac96fad22665a3d4990de7be00d75599b / 172.25.45.12:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 70c810be2883c2bcf06888cd7214b6c8aacbe78b / 172.26.163.95:6379 == migrated 1 slots Waiting for cluster view to be consistent... ........== 75e4cb42261db101de90d325859fb6d789e7c22a / 172.26.188.9:6379 == migrated 1 slots Waiting for cluster view to be consistent... ..........== 4385920f7dad41b96cf44503eff514b40dbc2eac / 172.26.235.91:6379 == migrated 1 slots Waiting for cluster view to be consistent... .......2020-02-26 02:34:37 ERROR timeout exceeded ~ $ ~ $ ~ $ rcc reshard --redis_url redis://172.24.222.56:6379 --timeout 30 file descriptors ulimit: 1048576 resharding can be hungry, bump it with ulimit -n if needed == b77fd799122312b936e99e7038cfd34d8a30f7f0 / 172.25.189.166:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 044ccfbac96fad22665a3d4990de7be00d75599b / 172.25.45.12:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 70c810be2883c2bcf06888cd7214b6c8aacbe78b / 172.26.163.95:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 75e4cb42261db101de90d325859fb6d789e7c22a / 172.26.188.9:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 4385920f7dad41b96cf44503eff514b40dbc2eac / 172.26.235.91:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 6de40bb7f8eab4e68d806ddbd5835c74340ac06f / 172.26.42.98:6379 == migrated 1 slots Waiting for cluster view to be consistent... ...............== 03d5101dd3c8139d48670c4cf6137205fa80d59d / 172.26.46.201:6379 == migrating 2 keys migrated 6 slots Waiting for cluster view to be consistent... ........== d99de160cc86f8ac9713116884329cb902e6fcd2 / 172.27.164.230:6379 == migrated 4 slots Waiting for cluster view to be consistent... .........== 9ef5e047a3f570e67a06871058bb2e54c2600acb / 172.27.98.220:6379 == migrated 6 slots Waiting for cluster view to be consistent... .......== 4cde2690f32644877c96e59f13bd72f44022f20f / 172.28.115.173:6379 == migrated 7 slots Waiting for cluster view to be consistent... ...........2020-02-26 02:35:26 ERROR timeout exceeded ~ $ ~ $ ~ $ ~ $ ~ $ rcc reshard --redis_url redis://172.24.222.56:6379 --timeout 45 file descriptors ulimit: 1048576 resharding can be hungry, bump it with ulimit -n if needed == b77fd799122312b936e99e7038cfd34d8a30f7f0 / 172.25.189.166:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 044ccfbac96fad22665a3d4990de7be00d75599b / 172.25.45.12:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 70c810be2883c2bcf06888cd7214b6c8aacbe78b / 172.26.163.95:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 75e4cb42261db101de90d325859fb6d789e7c22a / 172.26.188.9:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 4385920f7dad41b96cf44503eff514b40dbc2eac / 172.26.235.91:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 6de40bb7f8eab4e68d806ddbd5835c74340ac06f / 172.26.42.98:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 03d5101dd3c8139d48670c4cf6137205fa80d59d / 172.26.46.201:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== d99de160cc86f8ac9713116884329cb902e6fcd2 / 172.27.164.230:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 9ef5e047a3f570e67a06871058bb2e54c2600acb / 172.27.98.220:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 4cde2690f32644877c96e59f13bd72f44022f20f / 172.28.115.173:6379 == migrated 0 slots Waiting for cluster view to be consistent... .total migrated slots: 0 ~ $ ~ $ ~ $ ~ $ ~ $ ~ $ rcc cluster-check --redis_url redis://172.24.222.56:6379 cluster ok ~ $ ~ $ ~ $ ~ $ rcc cluster-nodes --redis_url redis://172.24.222.56:6379 Usage: rcc cluster-nodes [OPTIONS] Try \"rcc cluster-nodes --help\" for help. Error: no such option: --redis_url Did you mean --redis_urls? ~ $ rcc cluster-nodes --redis_urls redis://172.24.222.56:6379 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4261 4263-4279 4281-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-5707 5709-6305 6307-6504 6506-6553 11364 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6505 6554-7285 7287-7431 7433-7723 7725-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 7432 8192-8282 8284-9091 9093-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 445 9830-10695 10697-10819 10821-11039 11041-11107 11109-11184 11186-11363 11365-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 15288 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 943 3145 5708 7724 13107-14745 15198 15728 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 999 7286 10820 11108 14746-14996 14998-15197 15199-15287 15289-15727 15729-15840 15842-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-398 400-444 446-649 651-942 944-998 1000-1637 1649 6306 8283 9092 11185 14997 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 399 650 1638-1648 1650-3144 3146-3276 4262 4280 10696 11040 15841 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave","title":"Cluster Ips"},{"location":"cluster_ips/#cluster-ips","text":"","title":"Cluster Ips"},{"location":"cluster_ips/#problem","text":"Sometimes a redis cluster node dies, and kubernetes restarts it for you. And when this happens sometimes your pod will get a new ip address. However Redis read the cluster config from a file, nodes.conf, and that file could have the old value prior to the restart.","title":"Problem"},{"location":"cluster_ips/#solution","text":"","title":"Solution"},{"location":"cluster_ips/#the-nice-way","text":"This can be fixed automatically, so that when redis start it updates its ip address in nodes.conf with the one currently assigned. A nice way to do that is explained in this guide . Essentially we use a configmap that contain the script, and use that to start redis-server. An environment variable containing the pod ip is setup with this magic, in the deployment config file. env: - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP The shell script regexp is from the update-node.sh. #!/bin/sh REDIS_NODES=\"/data/nodes.conf\" sed -i -e \"/myself/ s/[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}\\.[0-9]\\{1,3\\}/${POD_IP}/\" ${REDIS_NODES} exec \"$@\" Don't ask me to debug this regexp :)","title":"The nice way"},{"location":"cluster_ips/#the-lame-way","text":"However sometimes you just want to fix things manually, before you automate them. To do that it is as simple as editing the script with an editor, and shutting down the node with redis-cli SHUTDOWN . I figured this out thanks to the new rcc cluster-check command, while trying to reshard my cluster. ~ $ rcc Usage: rcc [OPTIONS] COMMAND [ARGS]... RCC Rcc is a Redis Cluster Client Options: --version Show the version and exit. -v, --verbose --help Show this message and exit. Commands: keyspace Subscribe to a channel rcc keyspace... cli cli tool similar to redis-cli cluster-check Similar to redis-cli --cluster check Make sure all nodes... cluster-info Monitor redis metrics rcc cluster-info --stats... cluster-init cluster-nodes Monitor redis metrics rcc cluster-nodes endpoints Print endpoints associated with a redis cluster service make-cluster migrate publish Publish to a channel reshard Reshard using the bin-packing technique sub Subscribe to a channel rcc sub --redis_url... subscribe Subscribe to a channel rcc subscribe --redis_url... ~ $ rcc --version rcc, version 0.7.4 ~ $ rcc cluster-check --redis_url redis://172.24.222.56:6379 cluster unhealthy. Re-run with -v To get details you can rerun with the verbose flag, whose alias is -v. Then it's a bit painful but you'll need to visually scan for the nodes whose signature are different. This could be because slots assignments are differents between nodes, or if slave/masters assignments differs. ~ $ rcc -v cluster-check --redis_url redis://172.24.222.56:6379 2020-02-26 01:58:53 INFO redis://172.24.222.56:6379 5f9bcb66a8e67859342aa614082f1b24 balanced True coverage True 2020-02-26 01:58:53 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave ... output trimmed for readability 2020-02-26 01:58:53 INFO redis://172.27.70.50:6379 5f9bcb66a8e67859342aa614082f1b24 balanced True coverage True 2020-02-26 01:58:53 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave 2020-02-26 01:58:53 INFO redis://172.27.98.220:6379 bad1c3a12e48191a830d21cc9f855ed7 balanced True coverage True 2020-02-26 01:58:53 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.217:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave 2020-02-26 01:58:53 INFO redis://172.28.115.173:6379 5f9bcb66a8e67859342aa614082f1b24 balanced True coverage True 2020-02-26 01:58:53 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave ... 2020-02-26 01:58:53 INFO 2 unique signatures cluster unhealthy. Re-run with -v ~ $ First we need to know which node has the broken ip, so that we can connect to it. I'll use oc get -o yaml endpoints redis-cluster - ip: 172.27.98.220 nodeName: blah-blah-blah.example.com targetRef: kind: Pod name: redis-cluster-2-sl2wf namespace: cobra-live resourceVersion: \"1568323619\" uid: 4e174e94-4143-11ea-b4fb-eeeeeeeeeeee Now I know I need to rsh to redis-cluster-2-sl2wf . Once we've done that, we'll just need some vim/insert your favorite editor magic, to use the '172.27.98.220' address for the 'myself' node (the one where we are connected). The node will be rebooted, and we can ssh to it and grep for the new ip address. root@redis-cluster-2-sl2wf:/data# grep 220 nodes.conf 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379@16379 myself,master - 0 1582064573000 22 connected 0-1637 It's the right now. Let's re-run the cluster-check command, it will happilly succeed. ~ $ ~ $ ~ $ ~ $ ~ $ rcc -v cluster-check --redis_url redis://172.24.222.56:6379 2020-02-26 02:11:44 INFO redis://172.24.222.56:6379 5f9bcb66a8e67859342aa614082f1b24 balanced True coverage True 2020-02-26 02:11:44 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave ... edited for clarity ... 2020-02-26 02:11:45 INFO redis://172.30.99.146:6379 5f9bcb66a8e67859342aa614082f1b24 balanced True coverage True 2020-02-26 02:11:45 INFO 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-6553 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6554-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 8192-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 9830-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 13107-14745 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 14746-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-1637 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 1638-3276 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave 2020-02-26 02:11:45 INFO 1 unique signatures cluster ok ~ $ ~ $ Hooray ! Now we can finally reshard the cluster since it is in a good state.","title":"The lame way"},{"location":"cluster_ips/#resharding","text":"~ $ ~ $ rcc reshard --redis_url redis://172.24.222.56:6379 file descriptors ulimit: 1048576 resharding can be hungry, bump it with ulimit -n if needed == b77fd799122312b936e99e7038cfd34d8a30f7f0 / 172.25.189.166:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 044ccfbac96fad22665a3d4990de7be00d75599b / 172.25.45.12:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 70c810be2883c2bcf06888cd7214b6c8aacbe78b / 172.26.163.95:6379 == migrated 1 slots Waiting for cluster view to be consistent... ........== 75e4cb42261db101de90d325859fb6d789e7c22a / 172.26.188.9:6379 == migrated 1 slots Waiting for cluster view to be consistent... ..........== 4385920f7dad41b96cf44503eff514b40dbc2eac / 172.26.235.91:6379 == migrated 1 slots Waiting for cluster view to be consistent... .......2020-02-26 02:34:37 ERROR timeout exceeded ~ $ ~ $ ~ $ rcc reshard --redis_url redis://172.24.222.56:6379 --timeout 30 file descriptors ulimit: 1048576 resharding can be hungry, bump it with ulimit -n if needed == b77fd799122312b936e99e7038cfd34d8a30f7f0 / 172.25.189.166:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 044ccfbac96fad22665a3d4990de7be00d75599b / 172.25.45.12:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 70c810be2883c2bcf06888cd7214b6c8aacbe78b / 172.26.163.95:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 75e4cb42261db101de90d325859fb6d789e7c22a / 172.26.188.9:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 4385920f7dad41b96cf44503eff514b40dbc2eac / 172.26.235.91:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 6de40bb7f8eab4e68d806ddbd5835c74340ac06f / 172.26.42.98:6379 == migrated 1 slots Waiting for cluster view to be consistent... ...............== 03d5101dd3c8139d48670c4cf6137205fa80d59d / 172.26.46.201:6379 == migrating 2 keys migrated 6 slots Waiting for cluster view to be consistent... ........== d99de160cc86f8ac9713116884329cb902e6fcd2 / 172.27.164.230:6379 == migrated 4 slots Waiting for cluster view to be consistent... .........== 9ef5e047a3f570e67a06871058bb2e54c2600acb / 172.27.98.220:6379 == migrated 6 slots Waiting for cluster view to be consistent... .......== 4cde2690f32644877c96e59f13bd72f44022f20f / 172.28.115.173:6379 == migrated 7 slots Waiting for cluster view to be consistent... ...........2020-02-26 02:35:26 ERROR timeout exceeded ~ $ ~ $ ~ $ ~ $ ~ $ rcc reshard --redis_url redis://172.24.222.56:6379 --timeout 45 file descriptors ulimit: 1048576 resharding can be hungry, bump it with ulimit -n if needed == b77fd799122312b936e99e7038cfd34d8a30f7f0 / 172.25.189.166:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 044ccfbac96fad22665a3d4990de7be00d75599b / 172.25.45.12:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 70c810be2883c2bcf06888cd7214b6c8aacbe78b / 172.26.163.95:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 75e4cb42261db101de90d325859fb6d789e7c22a / 172.26.188.9:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 4385920f7dad41b96cf44503eff514b40dbc2eac / 172.26.235.91:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 6de40bb7f8eab4e68d806ddbd5835c74340ac06f / 172.26.42.98:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 03d5101dd3c8139d48670c4cf6137205fa80d59d / 172.26.46.201:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== d99de160cc86f8ac9713116884329cb902e6fcd2 / 172.27.164.230:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 9ef5e047a3f570e67a06871058bb2e54c2600acb / 172.27.98.220:6379 == migrated 0 slots Waiting for cluster view to be consistent... .== 4cde2690f32644877c96e59f13bd72f44022f20f / 172.28.115.173:6379 == migrated 0 slots Waiting for cluster view to be consistent... .total migrated slots: 0 ~ $ ~ $ ~ $ ~ $ ~ $ ~ $ rcc cluster-check --redis_url redis://172.24.222.56:6379 cluster ok ~ $ ~ $ ~ $ ~ $ rcc cluster-nodes --redis_url redis://172.24.222.56:6379 Usage: rcc cluster-nodes [OPTIONS] Try \"rcc cluster-nodes --help\" for help. Error: no such option: --redis_url Did you mean --redis_urls? ~ $ rcc cluster-nodes --redis_urls redis://172.24.222.56:6379 46e2fe542420f0adf7164d828e4e8e7b2726d90f 172.24.222.56:6379 slave 10767e9f3de8cc7a052c8a9146eb350d116910c1 172.24.250.232:6379 slave b77fd799122312b936e99e7038cfd34d8a30f7f0 172.25.189.166:6379 master 3277-4261 4263-4279 4281-4914 044ccfbac96fad22665a3d4990de7be00d75599b 172.25.45.12:6379 master 4915-5707 5709-6305 6307-6504 6506-6553 11364 70c810be2883c2bcf06888cd7214b6c8aacbe78b 172.26.163.95:6379 master 6505 6554-7285 7287-7431 7433-7723 7725-8191 75e4cb42261db101de90d325859fb6d789e7c22a 172.26.188.9:6379 master 7432 8192-8282 8284-9091 9093-9829 4385920f7dad41b96cf44503eff514b40dbc2eac 172.26.235.91:6379 master 445 9830-10695 10697-10819 10821-11039 11041-11107 11109-11184 11186-11363 11365-11468 6de40bb7f8eab4e68d806ddbd5835c74340ac06f 172.26.42.98:6379 master 11469-13106 15288 03d5101dd3c8139d48670c4cf6137205fa80d59d 172.26.46.201:6379 master 943 3145 5708 7724 13107-14745 15198 15728 d99de160cc86f8ac9713116884329cb902e6fcd2 172.27.164.230:6379 master 999 7286 10820 11108 14746-14996 14998-15197 15199-15287 15289-15727 15729-15840 15842-16383 2293aea0272bd7bc87c56afdc77b6f545cb01fc8 172.27.70.50:6379 slave 9ef5e047a3f570e67a06871058bb2e54c2600acb 172.27.98.220:6379 master 0-398 400-444 446-649 651-942 944-998 1000-1637 1649 6306 8283 9092 11185 14997 4cde2690f32644877c96e59f13bd72f44022f20f 172.28.115.173:6379 master 399 650 1638-1648 1650-3144 3146-3276 4262 4280 10696 11040 15841 e3534818adecb91e85e64859545ae17e65c35733 172.28.225.191:6379 slave 592fac9e124a634d0fb485147a89e13767c1a02f 172.28.53.125:6379 slave 08ba1ddb442e887732aa1b2502c4dda1dcd42acc 172.28.93.57:6379 slave 9e66ffc57e7df1d2ed4148067846f8358a1b2c70 172.29.113.236:6379 slave 066f891a0d9db6ffd71647e9126f023080bcbb41 172.29.203.50:6379 slave 2af270c467f97e7f4af24753ea26de97dfdf5c78 172.29.56.110:6379 slave b6af95ea28ec9e8b44f1697aea6475dbc446453f 172.30.99.146:6379 slave","title":"Resharding"},{"location":"cluster_testbed/","text":"Cluster test bed Problem It takes quite a few command to start a new cluster locally for testing. The redis source contains a simple shell script to help, but we go a little further. Solution $ rcc make-cluster --help Usage: rcc make-cluster [OPTIONS] Create, configure, initialize and run a redis cluster and a redis cluster proxy Options: --size INTEGER --start_port INTEGER -a, --password TEXT --help Show this message and exit. rcc make-cluster will create a temp folder, a set of redis-server config files and initialize the cluster using the redis-cli --cluster init command. Then it will run all the servers thank to honcho , which is a simple python init.d like tool based on Foreman . Foreman is the original Ruby init.d like tool that reads a very simple text file called a Procfile, containing a list of label:commands separated by new lines. $ cd /var/folders/qz/cb1zd5756hnd2tykv7z5sn_j8408d8/T/tmp03u8opux $ cat Procfile server0: redis-server server0.conf --protected-mode no --cluster-enabled yes --port 11000 server1: redis-server server1.conf --protected-mode no --cluster-enabled yes --port 11001 server2: redis-server server2.conf --protected-mode no --cluster-enabled yes --port 11002 server3: redis-server server3.conf --protected-mode no --cluster-enabled yes --port 11003 server4: redis-server server4.conf --protected-mode no --cluster-enabled yes --port 11004 server5: redis-server server5.conf --protected-mode no --cluster-enabled yes --port 11005 proxy: while test ! -f $ROOT/redis_cluster_ready ; do sleep 3 ; echo \"waiting for cluster to be up to start proxy\" ; done ; redis-cluster-proxy --port 11006 127.0.0.1:11000 127.0.0.1:11001 127.0.0.1:11002 127.0.0.1:11003 127.0.0.1:11004 127.0.0.1:11005 A Procfile is similzr to a Makefile. If you enter a folder with a Procfile and type honcho or make , all the referenced commands will be executed. Each redis server gets a minimal config file generated, and is started in cluster mode. The default port assigned to the first instance is 11000, and that can be configured. The number of cluster in the node can also be configured. $ cat server0.conf cluster-config-file nodes-0.conf dbfilename dump0.rdb Once the command has terminated (takes about 10 seconds on my mac), you can run redis-cli to test it. $ redis-cli -c -p 11000 127.0.0.1:11000> set foo bar -> Redirected to slot [12182] located at 127.0.0.1:11002 OK redis-cluster-proxy Redis cluster proxy is a proxy that let a redis client that is not cluster aware talk to a redis cluster regardless. The proxy handle all the cluster semantic transparently. This is extremely useful if you are trying to port an application which does not have a complete redis library. aioredis is an example of a widely used python library that does not support redis-cluster. The last line in the Procfile starts a redis cluster proxy instance, and configures it to point to our redis cluster. By default it runs on port 11006 . proxy: while test ! -f $ROOT/redis_cluster_ready ; do sleep 3 ; echo \"waiting for cluster to be up to start proxy\" ; done ; redis-cluster-proxy --port 11006 127.0.0.1:11000 127.0.0.1:11001 127.0.0.1:11002 127.0.0.1:11003 127.0.0.1:11004 127.0.0.1:11005 The odd line with the while expression is there to start the proxy only once the cluster has been setup. A simplified version of it is written below. $ while test ! -f /tmp/bar ; do sleep 1 ; echo waiting ; done ; echo READY waiting waiting (touch /tmp/bar in a different terminal) READY You can read/write to the cluster through the proxy using redis-cli at port 11006. $ redis-cli -p 11006 127.0.0.1:11006> set foo bar OK The project is super active and reaching v1 soon, it is still in beta but we use it and it works great. Full startup log (venv) rcc$ rcc make-cluster 1/6 Creating server config for range [11000, 11001, 11002, 11003, 11004, 11005] 2/6 Check that ports are opened ...... 3/6 Configuring and running 4/6 Wait for the cluster nodes to be running Checking redis://localhost:11000 .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:01 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) 19:12:01 system | server2.1 started (pid=72704) 19:12:01 server2.1 | 72704:C 09 Apr 2020 19:12:01.130 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server2.1 | 72704:C 09 Apr 2020 19:12:01.130 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72704, just started 19:12:01 server2.1 | 72704:C 09 Apr 2020 19:12:01.130 # Configuration loaded 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.131 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.132 * No cluster configuration found, I'm 23b57926eec91d51dd946f74bd3200ef5ca1c5e0 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.132 * Running mode=cluster, port=11002. 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.132 # Server initialized 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.132 * Ready to accept connections .2020-04-09 19:12:01 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) 19:12:01 system | proxy.1 started (pid=72705) 19:12:01 system | server0.1 started (pid=72709) 19:12:01 system | server5.1 started (pid=72707) 19:12:01 system | server1.1 started (pid=72708) 19:12:01 system | server3.1 started (pid=72706) 19:12:01 system | server4.1 started (pid=72710) 19:12:01 server3.1 | 72706:C 09 Apr 2020 19:12:01.345 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server3.1 | 72706:C 09 Apr 2020 19:12:01.345 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72706, just started 19:12:01 server3.1 | 72706:C 09 Apr 2020 19:12:01.345 # Configuration loaded 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.347 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server5.1 | 72707:C 09 Apr 2020 19:12:01.347 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server5.1 | 72707:C 09 Apr 2020 19:12:01.347 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72707, just started 19:12:01 server5.1 | 72707:C 09 Apr 2020 19:12:01.347 # Configuration loaded 19:12:01 server0.1 | 72709:C 09 Apr 2020 19:12:01.347 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server0.1 | 72709:C 09 Apr 2020 19:12:01.347 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72709, just started 19:12:01 server0.1 | 72709:C 09 Apr 2020 19:12:01.347 # Configuration loaded 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.348 * No cluster configuration found, I'm faf021cd1c2e7eeea1ed121dead0402b889bf41c 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.349 * Running mode=cluster, port=11003. 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.349 # Server initialized 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.349 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.350 * No cluster configuration found, I'm cd880f1392726db05f268ed9d5990ac9ff4d5cd2 19:12:01 server1.1 | 72708:C 09 Apr 2020 19:12:01.348 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server1.1 | 72708:C 09 Apr 2020 19:12:01.348 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72708, just started 19:12:01 server1.1 | 72708:C 09 Apr 2020 19:12:01.348 # Configuration loaded 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.350 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.351 * Running mode=cluster, port=11005. 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.351 # Server initialized 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.349 * Ready to accept connections 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.349 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.351 * No cluster configuration found, I'm 6940667737386545f0fd396b4d7d0b100f4fdfaf 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.352 * Ready to accept connections 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.351 * No cluster configuration found, I'm c66e96d1dce3d857f8a007c4727fe807c537a143 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.353 * Running mode=cluster, port=11001. 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.352 * Running mode=cluster, port=11000. 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.353 # Server initialized 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.352 # Server initialized 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.352 * Ready to accept connections 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.353 * Ready to accept connections 19:12:01 server4.1 | 72710:C 09 Apr 2020 19:12:01.348 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server4.1 | 72710:C 09 Apr 2020 19:12:01.348 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72710, just started 19:12:01 server4.1 | 72710:C 09 Apr 2020 19:12:01.348 # Configuration loaded 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.350 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.352 * No cluster configuration found, I'm 00d6302ad615c460222e147b31c97a361412261c 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.353 * Running mode=cluster, port=11004. 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.353 # Server initialized 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.353 * Ready to accept connections . Checking redis://localhost:11001 . Checking redis://localhost:11002 . 5/6 Initialize the cluster >>> Performing hash slots allocation on 6 nodes... Master[0] -> Slots 0 - 5460 Master[1] -> Slots 5461 - 10922 Master[2] -> Slots 10923 - 16383 Adding replica 127.0.0.1:11004 to 127.0.0.1:11000 Adding replica 127.0.0.1:11005 to 127.0.0.1:11001 Adding replica 127.0.0.1:11003 to 127.0.0.1:11002 >>> Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: c66e96d1dce3d857f8a007c4727fe807c537a143 127.0.0.1:11000 slots:[0-5460] (5461 slots) master M: 6940667737386545f0fd396b4d7d0b100f4fdfaf 127.0.0.1:11001 slots:[5461-10922] (5462 slots) master M: 23b57926eec91d51dd946f74bd3200ef5ca1c5e0 127.0.0.1:11002 slots:[10923-16383] (5461 slots) master S: faf021cd1c2e7eeea1ed121dead0402b889bf41c 127.0.0.1:11003 replicates 23b57926eec91d51dd946f74bd3200ef5ca1c5e0 S: 00d6302ad615c460222e147b31c97a361412261c 127.0.0.1:11004 replicates c66e96d1dce3d857f8a007c4727fe807c537a143 S: cd880f1392726db05f268ed9d5990ac9ff4d5cd2 127.0.0.1:11005 replicates 6940667737386545f0fd396b4d7d0b100f4fdfaf Can I set the above configuration? (type 'yes' to accept): >>> Nodes configuration updated >>> Assign a different config epoch to each node 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.446 # configEpoch set to 1 via CLUSTER SET-CONFIG-EPOCH 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.447 # configEpoch set to 2 via CLUSTER SET-CONFIG-EPOCH 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.447 # configEpoch set to 3 via CLUSTER SET-CONFIG-EPOCH 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.447 # configEpoch set to 4 via CLUSTER SET-CONFIG-EPOCH 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.447 # configEpoch set to 5 via CLUSTER SET-CONFIG-EPOCH >>> Sending CLUSTER MEET messages to join the cluster 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.448 # configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.451 # IP address for this node updated to 127.0.0.1 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.453 # IP address for this node updated to 127.0.0.1 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.554 # IP address for this node updated to 127.0.0.1 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.554 # IP address for this node updated to 127.0.0.1 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.554 # IP address for this node updated to 127.0.0.1 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.554 # IP address for this node updated to 127.0.0.1 Waiting for the cluster to join .19:12:03 server1.1 | 72708:M 09 Apr 2020 19:12:03.391 # Cluster state changed: ok 19:12:03 server0.1 | 72709:M 09 Apr 2020 19:12:03.391 # Cluster state changed: ok .19:12:04 server5.1 | 72707:M 09 Apr 2020 19:12:04.310 # Cluster state changed: ok 19:12:04 proxy.1 | waiting for cluster to be up to start proxy .19:12:05 server4.1 | 72710:M 09 Apr 2020 19:12:05.329 # Cluster state changed: ok 19:12:05 server2.1 | 72704:M 09 Apr 2020 19:12:05.337 # Cluster state changed: ok ..19:12:07 proxy.1 | waiting for cluster to be up to start proxy ..19:12:09 server3.1 | 72706:M 09 Apr 2020 19:12:09.208 # Cluster state changed: ok 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.486 * Before turning into a replica, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer. 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.487 * Before turning into a replica, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer. 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.487 * Before turning into a replica, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer. >>> Performing Cluster Check (using node 127.0.0.1:11000) M: c66e96d1dce3d857f8a007c4727fe807c537a143 127.0.0.1:11000 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: cd880f1392726db05f268ed9d5990ac9ff4d5cd2 127.0.0.1:11005 slots: (0 slots) slave replicates 6940667737386545f0fd396b4d7d0b100f4fdfaf M: 23b57926eec91d51dd946f74bd3200ef5ca1c5e0 127.0.0.1:11002 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: 00d6302ad615c460222e147b31c97a361412261c 127.0.0.1:11004 slots: (0 slots) slave replicates c66e96d1dce3d857f8a007c4727fe807c537a143 M: 6940667737386545f0fd396b4d7d0b100f4fdfaf 127.0.0.1:11001 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: faf021cd1c2e7eeea1ed121dead0402b889bf41c 127.0.0.1:11003 slots: (0 slots) slave replicates 23b57926eec91d51dd946f74bd3200ef5ca1c5e0 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 6/6 Wait for all cluster nodes to be consistent 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.511 * Connecting to MASTER 127.0.0.1:11002 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.511 * MASTER <-> REPLICA sync started 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.511 * Non blocking connect for SYNC fired the event. 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.511 * Master replied to PING, replication can continue... 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.512 * Trying a partial resynchronization (request 4df041657f7f8594ec0a302c0c41118dc08777f0:1). 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.512 * Replica 127.0.0.1:11003 asks for synchronization 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.512 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '4df041657f7f8594ec0a302c0c41118dc08777f0', my replication IDs are 'b3a87de48fc894761c5380d18f2c81437d35235f' and '0000000000000000000000000000000000000000') 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.512 * Starting BGSAVE for SYNC with target: disk 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.512 * Background saving started by pid 72722 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.512 * Full resync from master: b78b16d46be8d42cc78c1c4b3e7d4ec445c30d03:0 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.512 * Discarding previously cached master state. 19:12:09 server2.1 | 72722:C 09 Apr 2020 19:12:09.513 * DB saved on disk 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.523 * Connecting to MASTER 127.0.0.1:11000 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.523 * Connecting to MASTER 127.0.0.1:11001 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.523 * MASTER <-> REPLICA sync started 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.523 * MASTER <-> REPLICA sync started 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.523 * Non blocking connect for SYNC fired the event. 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.523 * Non blocking connect for SYNC fired the event. 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.523 * Master replied to PING, replication can continue... 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.523 * Master replied to PING, replication can continue... 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.523 * Trying a partial resynchronization (request a1362164e84c8a176cee42ed71880e416362a279:1). 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.523 * Replica 127.0.0.1:11004 asks for synchronization 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.523 * Replica 127.0.0.1:11005 asks for synchronization 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.523 * Trying a partial resynchronization (request 5a35ba14e6e6fe123985315c665ffbeacea69c2d:1). 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.523 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for 'a1362164e84c8a176cee42ed71880e416362a279', my replication IDs are 'b826b09ebf81696089a7fed8debe307c07d03755' and '0000000000000000000000000000000000000000') 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.523 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '5a35ba14e6e6fe123985315c665ffbeacea69c2d', my replication IDs are '5f1b4ac6e693247d2ac267ae9af32d0a53902532' and '0000000000000000000000000000000000000000') 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.523 * Starting BGSAVE for SYNC with target: disk 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.523 * Starting BGSAVE for SYNC with target: disk 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.524 * Background saving started by pid 72723 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.524 * Full resync from master: 50025f2dcd1f778f36018a32ad70817d5143a5c6:0 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.524 * Background saving started by pid 72724 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.524 * Full resync from master: 07446ccb851df269a57777a77388c4a724e4e48e:0 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.524 * Discarding previously cached master state. 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.524 * Discarding previously cached master state. 19:12:09 server1.1 | 72724:C 09 Apr 2020 19:12:09.525 * DB saved on disk 19:12:09 server0.1 | 72723:C 09 Apr 2020 19:12:09.525 * DB saved on disk Waiting for cluster to be consistent... 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.615 * Background saving terminated with success 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.616 * Synchronization with replica 127.0.0.1:11003 succeeded 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.616 * MASTER <-> REPLICA sync: receiving 175 bytes from master 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.616 * MASTER <-> REPLICA sync: Flushing old data 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.616 * MASTER <-> REPLICA sync: Loading DB in memory 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.616 * MASTER <-> REPLICA sync: Finished with success 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.620 * Background saving terminated with success 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.620 * MASTER <-> REPLICA sync: receiving 175 bytes from master 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.620 * Synchronization with replica 127.0.0.1:11004 succeeded 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.620 * MASTER <-> REPLICA sync: Flushing old data 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.620 * MASTER <-> REPLICA sync: Loading DB in memory 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.621 * MASTER <-> REPLICA sync: Finished with success 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.623 * Background saving terminated with success 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.623 * Synchronization with replica 127.0.0.1:11005 succeeded 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.623 * MASTER <-> REPLICA sync: receiving 175 bytes from master 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.623 * MASTER <-> REPLICA sync: Flushing old data 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.623 * MASTER <-> REPLICA sync: Loading DB in memory 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.624 * MASTER <-> REPLICA sync: Finished with success 19:12:10 proxy.1 | waiting for cluster to be up to start proxy Waiting for cluster to be consistent... Waiting for cluster to be consistent... Waiting for cluster to be consistent... 19:12:13 proxy.1 | waiting for cluster to be up to start proxy Waiting for cluster to be consistent... Cluster ready ! Config files created in folder /var/folders/qz/cb1zd5756hnd2tykv7z5sn_j8408d8/T/tmpdlq0lavp 19:12:16 proxy.1 | waiting for cluster to be up to start proxy 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Redis Cluster Proxy v999.999.999 (unstable) 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Commit: (eb092d0b/0) 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Git Branch: unstable 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] PID: 72735 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] OS: Darwin 19.3.0 x86_64 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Bits: 64 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Log level: info 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Connections pool size: 10 (respawn 2 every 50ms if below 10) 19:12:16 proxy.1 | [2020-04-09 19:12:16.382/M] Listening on *:11006 19:12:16 proxy.1 | [2020-04-09 19:12:16.382/M] Starting 8 threads... 19:12:16 proxy.1 | [2020-04-09 19:12:16.382/M] Fetching cluster configuration... 19:12:16 proxy.1 | [2020-04-09 19:12:16.386/M] Cluster Address: 127.0.0.1:11000 19:12:16 proxy.1 | [2020-04-09 19:12:16.386/M] Cluster has 3 masters and 3 replica(s) 19:12:16 proxy.1 | [2020-04-09 19:12:16.386/M] Increased maximum number of open files to 10518 (it was originally set to 1024). 19:12:16 proxy.1 | [2020-04-09 19:12:16.427/M] All thread(s) started!","title":"Cluster test bed"},{"location":"cluster_testbed/#cluster-test-bed","text":"","title":"Cluster test bed"},{"location":"cluster_testbed/#problem","text":"It takes quite a few command to start a new cluster locally for testing. The redis source contains a simple shell script to help, but we go a little further.","title":"Problem"},{"location":"cluster_testbed/#solution","text":"$ rcc make-cluster --help Usage: rcc make-cluster [OPTIONS] Create, configure, initialize and run a redis cluster and a redis cluster proxy Options: --size INTEGER --start_port INTEGER -a, --password TEXT --help Show this message and exit. rcc make-cluster will create a temp folder, a set of redis-server config files and initialize the cluster using the redis-cli --cluster init command. Then it will run all the servers thank to honcho , which is a simple python init.d like tool based on Foreman . Foreman is the original Ruby init.d like tool that reads a very simple text file called a Procfile, containing a list of label:commands separated by new lines. $ cd /var/folders/qz/cb1zd5756hnd2tykv7z5sn_j8408d8/T/tmp03u8opux $ cat Procfile server0: redis-server server0.conf --protected-mode no --cluster-enabled yes --port 11000 server1: redis-server server1.conf --protected-mode no --cluster-enabled yes --port 11001 server2: redis-server server2.conf --protected-mode no --cluster-enabled yes --port 11002 server3: redis-server server3.conf --protected-mode no --cluster-enabled yes --port 11003 server4: redis-server server4.conf --protected-mode no --cluster-enabled yes --port 11004 server5: redis-server server5.conf --protected-mode no --cluster-enabled yes --port 11005 proxy: while test ! -f $ROOT/redis_cluster_ready ; do sleep 3 ; echo \"waiting for cluster to be up to start proxy\" ; done ; redis-cluster-proxy --port 11006 127.0.0.1:11000 127.0.0.1:11001 127.0.0.1:11002 127.0.0.1:11003 127.0.0.1:11004 127.0.0.1:11005 A Procfile is similzr to a Makefile. If you enter a folder with a Procfile and type honcho or make , all the referenced commands will be executed. Each redis server gets a minimal config file generated, and is started in cluster mode. The default port assigned to the first instance is 11000, and that can be configured. The number of cluster in the node can also be configured. $ cat server0.conf cluster-config-file nodes-0.conf dbfilename dump0.rdb Once the command has terminated (takes about 10 seconds on my mac), you can run redis-cli to test it. $ redis-cli -c -p 11000 127.0.0.1:11000> set foo bar -> Redirected to slot [12182] located at 127.0.0.1:11002 OK","title":"Solution"},{"location":"cluster_testbed/#redis-cluster-proxy","text":"Redis cluster proxy is a proxy that let a redis client that is not cluster aware talk to a redis cluster regardless. The proxy handle all the cluster semantic transparently. This is extremely useful if you are trying to port an application which does not have a complete redis library. aioredis is an example of a widely used python library that does not support redis-cluster. The last line in the Procfile starts a redis cluster proxy instance, and configures it to point to our redis cluster. By default it runs on port 11006 . proxy: while test ! -f $ROOT/redis_cluster_ready ; do sleep 3 ; echo \"waiting for cluster to be up to start proxy\" ; done ; redis-cluster-proxy --port 11006 127.0.0.1:11000 127.0.0.1:11001 127.0.0.1:11002 127.0.0.1:11003 127.0.0.1:11004 127.0.0.1:11005 The odd line with the while expression is there to start the proxy only once the cluster has been setup. A simplified version of it is written below. $ while test ! -f /tmp/bar ; do sleep 1 ; echo waiting ; done ; echo READY waiting waiting (touch /tmp/bar in a different terminal) READY You can read/write to the cluster through the proxy using redis-cli at port 11006. $ redis-cli -p 11006 127.0.0.1:11006> set foo bar OK The project is super active and reaching v1 soon, it is still in beta but we use it and it works great.","title":"redis-cluster-proxy"},{"location":"cluster_testbed/#full-startup-log","text":"(venv) rcc$ rcc make-cluster 1/6 Creating server config for range [11000, 11001, 11002, 11003, 11004, 11005] 2/6 Check that ports are opened ...... 3/6 Configuring and running 4/6 Wait for the cluster nodes to be running Checking redis://localhost:11000 .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:00 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) .2020-04-09 19:12:01 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) 19:12:01 system | server2.1 started (pid=72704) 19:12:01 server2.1 | 72704:C 09 Apr 2020 19:12:01.130 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server2.1 | 72704:C 09 Apr 2020 19:12:01.130 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72704, just started 19:12:01 server2.1 | 72704:C 09 Apr 2020 19:12:01.130 # Configuration loaded 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.131 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.132 * No cluster configuration found, I'm 23b57926eec91d51dd946f74bd3200ef5ca1c5e0 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.132 * Running mode=cluster, port=11002. 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.132 # Server initialized 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.132 * Ready to accept connections .2020-04-09 19:12:01 WARNING Multiple exceptions: [Errno 61] Connect call failed ('::1', 11000, 0, 0), [Errno 61] Connect call failed ('127.0.0.1', 11000) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/subprocess.py:844: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used self.stdout = io.open(c2pread, 'rb', bufsize) 19:12:01 system | proxy.1 started (pid=72705) 19:12:01 system | server0.1 started (pid=72709) 19:12:01 system | server5.1 started (pid=72707) 19:12:01 system | server1.1 started (pid=72708) 19:12:01 system | server3.1 started (pid=72706) 19:12:01 system | server4.1 started (pid=72710) 19:12:01 server3.1 | 72706:C 09 Apr 2020 19:12:01.345 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server3.1 | 72706:C 09 Apr 2020 19:12:01.345 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72706, just started 19:12:01 server3.1 | 72706:C 09 Apr 2020 19:12:01.345 # Configuration loaded 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.347 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server5.1 | 72707:C 09 Apr 2020 19:12:01.347 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server5.1 | 72707:C 09 Apr 2020 19:12:01.347 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72707, just started 19:12:01 server5.1 | 72707:C 09 Apr 2020 19:12:01.347 # Configuration loaded 19:12:01 server0.1 | 72709:C 09 Apr 2020 19:12:01.347 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server0.1 | 72709:C 09 Apr 2020 19:12:01.347 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72709, just started 19:12:01 server0.1 | 72709:C 09 Apr 2020 19:12:01.347 # Configuration loaded 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.348 * No cluster configuration found, I'm faf021cd1c2e7eeea1ed121dead0402b889bf41c 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.349 * Running mode=cluster, port=11003. 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.349 # Server initialized 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.349 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.350 * No cluster configuration found, I'm cd880f1392726db05f268ed9d5990ac9ff4d5cd2 19:12:01 server1.1 | 72708:C 09 Apr 2020 19:12:01.348 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server1.1 | 72708:C 09 Apr 2020 19:12:01.348 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72708, just started 19:12:01 server1.1 | 72708:C 09 Apr 2020 19:12:01.348 # Configuration loaded 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.350 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.351 * Running mode=cluster, port=11005. 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.351 # Server initialized 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.349 * Ready to accept connections 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.349 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.351 * No cluster configuration found, I'm 6940667737386545f0fd396b4d7d0b100f4fdfaf 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.352 * Ready to accept connections 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.351 * No cluster configuration found, I'm c66e96d1dce3d857f8a007c4727fe807c537a143 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.353 * Running mode=cluster, port=11001. 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.352 * Running mode=cluster, port=11000. 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.353 # Server initialized 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.352 # Server initialized 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.352 * Ready to accept connections 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.353 * Ready to accept connections 19:12:01 server4.1 | 72710:C 09 Apr 2020 19:12:01.348 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 19:12:01 server4.1 | 72710:C 09 Apr 2020 19:12:01.348 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=72710, just started 19:12:01 server4.1 | 72710:C 09 Apr 2020 19:12:01.348 # Configuration loaded 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.350 * Increased maximum number of open files to 10032 (it was originally set to 1024). 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.352 * No cluster configuration found, I'm 00d6302ad615c460222e147b31c97a361412261c 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.353 * Running mode=cluster, port=11004. 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.353 # Server initialized 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.353 * Ready to accept connections . Checking redis://localhost:11001 . Checking redis://localhost:11002 . 5/6 Initialize the cluster >>> Performing hash slots allocation on 6 nodes... Master[0] -> Slots 0 - 5460 Master[1] -> Slots 5461 - 10922 Master[2] -> Slots 10923 - 16383 Adding replica 127.0.0.1:11004 to 127.0.0.1:11000 Adding replica 127.0.0.1:11005 to 127.0.0.1:11001 Adding replica 127.0.0.1:11003 to 127.0.0.1:11002 >>> Trying to optimize slaves allocation for anti-affinity [WARNING] Some slaves are in the same host as their master M: c66e96d1dce3d857f8a007c4727fe807c537a143 127.0.0.1:11000 slots:[0-5460] (5461 slots) master M: 6940667737386545f0fd396b4d7d0b100f4fdfaf 127.0.0.1:11001 slots:[5461-10922] (5462 slots) master M: 23b57926eec91d51dd946f74bd3200ef5ca1c5e0 127.0.0.1:11002 slots:[10923-16383] (5461 slots) master S: faf021cd1c2e7eeea1ed121dead0402b889bf41c 127.0.0.1:11003 replicates 23b57926eec91d51dd946f74bd3200ef5ca1c5e0 S: 00d6302ad615c460222e147b31c97a361412261c 127.0.0.1:11004 replicates c66e96d1dce3d857f8a007c4727fe807c537a143 S: cd880f1392726db05f268ed9d5990ac9ff4d5cd2 127.0.0.1:11005 replicates 6940667737386545f0fd396b4d7d0b100f4fdfaf Can I set the above configuration? (type 'yes' to accept): >>> Nodes configuration updated >>> Assign a different config epoch to each node 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.446 # configEpoch set to 1 via CLUSTER SET-CONFIG-EPOCH 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.447 # configEpoch set to 2 via CLUSTER SET-CONFIG-EPOCH 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.447 # configEpoch set to 3 via CLUSTER SET-CONFIG-EPOCH 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.447 # configEpoch set to 4 via CLUSTER SET-CONFIG-EPOCH 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.447 # configEpoch set to 5 via CLUSTER SET-CONFIG-EPOCH >>> Sending CLUSTER MEET messages to join the cluster 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.448 # configEpoch set to 6 via CLUSTER SET-CONFIG-EPOCH 19:12:01 server0.1 | 72709:M 09 Apr 2020 19:12:01.451 # IP address for this node updated to 127.0.0.1 19:12:01 server3.1 | 72706:M 09 Apr 2020 19:12:01.453 # IP address for this node updated to 127.0.0.1 19:12:01 server5.1 | 72707:M 09 Apr 2020 19:12:01.554 # IP address for this node updated to 127.0.0.1 19:12:01 server2.1 | 72704:M 09 Apr 2020 19:12:01.554 # IP address for this node updated to 127.0.0.1 19:12:01 server1.1 | 72708:M 09 Apr 2020 19:12:01.554 # IP address for this node updated to 127.0.0.1 19:12:01 server4.1 | 72710:M 09 Apr 2020 19:12:01.554 # IP address for this node updated to 127.0.0.1 Waiting for the cluster to join .19:12:03 server1.1 | 72708:M 09 Apr 2020 19:12:03.391 # Cluster state changed: ok 19:12:03 server0.1 | 72709:M 09 Apr 2020 19:12:03.391 # Cluster state changed: ok .19:12:04 server5.1 | 72707:M 09 Apr 2020 19:12:04.310 # Cluster state changed: ok 19:12:04 proxy.1 | waiting for cluster to be up to start proxy .19:12:05 server4.1 | 72710:M 09 Apr 2020 19:12:05.329 # Cluster state changed: ok 19:12:05 server2.1 | 72704:M 09 Apr 2020 19:12:05.337 # Cluster state changed: ok ..19:12:07 proxy.1 | waiting for cluster to be up to start proxy ..19:12:09 server3.1 | 72706:M 09 Apr 2020 19:12:09.208 # Cluster state changed: ok 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.486 * Before turning into a replica, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer. 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.487 * Before turning into a replica, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer. 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.487 * Before turning into a replica, using my master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer. >>> Performing Cluster Check (using node 127.0.0.1:11000) M: c66e96d1dce3d857f8a007c4727fe807c537a143 127.0.0.1:11000 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: cd880f1392726db05f268ed9d5990ac9ff4d5cd2 127.0.0.1:11005 slots: (0 slots) slave replicates 6940667737386545f0fd396b4d7d0b100f4fdfaf M: 23b57926eec91d51dd946f74bd3200ef5ca1c5e0 127.0.0.1:11002 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: 00d6302ad615c460222e147b31c97a361412261c 127.0.0.1:11004 slots: (0 slots) slave replicates c66e96d1dce3d857f8a007c4727fe807c537a143 M: 6940667737386545f0fd396b4d7d0b100f4fdfaf 127.0.0.1:11001 slots:[5461-10922] (5462 slots) master 1 additional replica(s) S: faf021cd1c2e7eeea1ed121dead0402b889bf41c 127.0.0.1:11003 slots: (0 slots) slave replicates 23b57926eec91d51dd946f74bd3200ef5ca1c5e0 [OK] All nodes agree about slots configuration. >>> Check for open slots... >>> Check slots coverage... [OK] All 16384 slots covered. 6/6 Wait for all cluster nodes to be consistent 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.511 * Connecting to MASTER 127.0.0.1:11002 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.511 * MASTER <-> REPLICA sync started 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.511 * Non blocking connect for SYNC fired the event. 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.511 * Master replied to PING, replication can continue... 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.512 * Trying a partial resynchronization (request 4df041657f7f8594ec0a302c0c41118dc08777f0:1). 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.512 * Replica 127.0.0.1:11003 asks for synchronization 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.512 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '4df041657f7f8594ec0a302c0c41118dc08777f0', my replication IDs are 'b3a87de48fc894761c5380d18f2c81437d35235f' and '0000000000000000000000000000000000000000') 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.512 * Starting BGSAVE for SYNC with target: disk 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.512 * Background saving started by pid 72722 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.512 * Full resync from master: b78b16d46be8d42cc78c1c4b3e7d4ec445c30d03:0 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.512 * Discarding previously cached master state. 19:12:09 server2.1 | 72722:C 09 Apr 2020 19:12:09.513 * DB saved on disk 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.523 * Connecting to MASTER 127.0.0.1:11000 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.523 * Connecting to MASTER 127.0.0.1:11001 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.523 * MASTER <-> REPLICA sync started 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.523 * MASTER <-> REPLICA sync started 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.523 * Non blocking connect for SYNC fired the event. 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.523 * Non blocking connect for SYNC fired the event. 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.523 * Master replied to PING, replication can continue... 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.523 * Master replied to PING, replication can continue... 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.523 * Trying a partial resynchronization (request a1362164e84c8a176cee42ed71880e416362a279:1). 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.523 * Replica 127.0.0.1:11004 asks for synchronization 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.523 * Replica 127.0.0.1:11005 asks for synchronization 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.523 * Trying a partial resynchronization (request 5a35ba14e6e6fe123985315c665ffbeacea69c2d:1). 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.523 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for 'a1362164e84c8a176cee42ed71880e416362a279', my replication IDs are 'b826b09ebf81696089a7fed8debe307c07d03755' and '0000000000000000000000000000000000000000') 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.523 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for '5a35ba14e6e6fe123985315c665ffbeacea69c2d', my replication IDs are '5f1b4ac6e693247d2ac267ae9af32d0a53902532' and '0000000000000000000000000000000000000000') 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.523 * Starting BGSAVE for SYNC with target: disk 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.523 * Starting BGSAVE for SYNC with target: disk 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.524 * Background saving started by pid 72723 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.524 * Full resync from master: 50025f2dcd1f778f36018a32ad70817d5143a5c6:0 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.524 * Background saving started by pid 72724 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.524 * Full resync from master: 07446ccb851df269a57777a77388c4a724e4e48e:0 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.524 * Discarding previously cached master state. 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.524 * Discarding previously cached master state. 19:12:09 server1.1 | 72724:C 09 Apr 2020 19:12:09.525 * DB saved on disk 19:12:09 server0.1 | 72723:C 09 Apr 2020 19:12:09.525 * DB saved on disk Waiting for cluster to be consistent... 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.615 * Background saving terminated with success 19:12:09 server2.1 | 72704:M 09 Apr 2020 19:12:09.616 * Synchronization with replica 127.0.0.1:11003 succeeded 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.616 * MASTER <-> REPLICA sync: receiving 175 bytes from master 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.616 * MASTER <-> REPLICA sync: Flushing old data 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.616 * MASTER <-> REPLICA sync: Loading DB in memory 19:12:09 server3.1 | 72706:S 09 Apr 2020 19:12:09.616 * MASTER <-> REPLICA sync: Finished with success 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.620 * Background saving terminated with success 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.620 * MASTER <-> REPLICA sync: receiving 175 bytes from master 19:12:09 server0.1 | 72709:M 09 Apr 2020 19:12:09.620 * Synchronization with replica 127.0.0.1:11004 succeeded 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.620 * MASTER <-> REPLICA sync: Flushing old data 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.620 * MASTER <-> REPLICA sync: Loading DB in memory 19:12:09 server4.1 | 72710:S 09 Apr 2020 19:12:09.621 * MASTER <-> REPLICA sync: Finished with success 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.623 * Background saving terminated with success 19:12:09 server1.1 | 72708:M 09 Apr 2020 19:12:09.623 * Synchronization with replica 127.0.0.1:11005 succeeded 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.623 * MASTER <-> REPLICA sync: receiving 175 bytes from master 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.623 * MASTER <-> REPLICA sync: Flushing old data 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.623 * MASTER <-> REPLICA sync: Loading DB in memory 19:12:09 server5.1 | 72707:S 09 Apr 2020 19:12:09.624 * MASTER <-> REPLICA sync: Finished with success 19:12:10 proxy.1 | waiting for cluster to be up to start proxy Waiting for cluster to be consistent... Waiting for cluster to be consistent... Waiting for cluster to be consistent... 19:12:13 proxy.1 | waiting for cluster to be up to start proxy Waiting for cluster to be consistent... Cluster ready ! Config files created in folder /var/folders/qz/cb1zd5756hnd2tykv7z5sn_j8408d8/T/tmpdlq0lavp 19:12:16 proxy.1 | waiting for cluster to be up to start proxy 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Redis Cluster Proxy v999.999.999 (unstable) 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Commit: (eb092d0b/0) 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Git Branch: unstable 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] PID: 72735 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] OS: Darwin 19.3.0 x86_64 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Bits: 64 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Log level: info 19:12:16 proxy.1 | [2020-04-09 19:12:16.381/M] Connections pool size: 10 (respawn 2 every 50ms if below 10) 19:12:16 proxy.1 | [2020-04-09 19:12:16.382/M] Listening on *:11006 19:12:16 proxy.1 | [2020-04-09 19:12:16.382/M] Starting 8 threads... 19:12:16 proxy.1 | [2020-04-09 19:12:16.382/M] Fetching cluster configuration... 19:12:16 proxy.1 | [2020-04-09 19:12:16.386/M] Cluster Address: 127.0.0.1:11000 19:12:16 proxy.1 | [2020-04-09 19:12:16.386/M] Cluster has 3 masters and 3 replica(s) 19:12:16 proxy.1 | [2020-04-09 19:12:16.386/M] Increased maximum number of open files to 10518 (it was originally set to 1024). 19:12:16 proxy.1 | [2020-04-09 19:12:16.427/M] All thread(s) started!","title":"Full startup log"},{"location":"resharding/","text":"Resharding Using rcc to reshard a redis cluster using keyspace notification rcc comes with 2 important tools, one for analyzing the keys access accross nodes, built on top of redis keyspace notifications (which in turns runs on top of redis PubSub). The instrumentation is done over a period of time to sample the key access patterns. A 'weights' file is created as part of this tool. That file is saved in the current working directory by default. It is a very simple csv file that show how often a key is accessed. $ head weights.csv _pubsub::foo,50 _pubsub::bar,53 _pubsub::baz,7 _pubsub::buz,19 _pubsub::blah,940 _pubsub::blooh,20 _pubsub::xxx,552 _pubsub::yyy,571 _pubsub::zzz,92 _pubsub::foo,5035 The second tool is used to reshard a cluster, and migrate slots to node using the bin-packing algorithm. To feed this algorithm, weights are required. Generating redis cluster traffic. We use the cobra publish in batch mode command to send data to cobra , which internally send lots of XADD commands to our redis cluster. Cobra server started with: cobra run -r redis://localhost:11000 Cobra publishers: cobra publish --batch The redis cluster is started with: rcc make-cluster ; rcc has a convenience sub-command to generate config file for cluster (by default 3 masters and 3 replicas), and finally initialize the cluster. Keyspace access analysis before resharding rcc keyspace --redis_url redis://localhost:11000 --timeout 10 ... == Nodes == # each \u220e represents a count of 105. total 15515 127.0.0.1:11000 [ 6789] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 127.0.0.1:11002 [ 4983] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 127.0.0.1:11001 [ 3743] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e Resharding $ rcc reshard --redis_url redis://localhost:11000 file descriptors ulimit: 1024 resharding can be hungry, bump it with ulimit -n if needed == f3fa13802f339abb98ccb377e8a1a4eb957be987 / 127.0.0.1:11000 == migrated 0 slots Waiting for cluster view to be consistent... .== 8c67b776ab52ad756777866dcb8425cc866c71a3 / 127.0.0.1:11001 == migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrated 10 slots Waiting for cluster view to be consistent... ............== d50246e7e3914639759add181c71a2e3c879ed2f / 127.0.0.1:11002 == migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrated 11 slots Waiting for cluster view to be consistent... .....total migrated slots: 11 It roughtly looks like we took slots away from the first node, and gave them to the other two redis instances. Keyspace access analysis after resharding rcc keyspace --redis_url redis://localhost:11000 --timeout 10 ... == Nodes == # each \u220e represents a count of 79. total 15114 127.0.0.1:11002 [ 5087] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 127.0.0.1:11000 [ 5040] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 127.0.0.1:11001 [ 4987] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e Larger Cobra cluster The cluster has 10 masters and 10 replicas. We sample about 5,000,000 received commands (this takes around a minute). This use the new --count option of the keyspace command. Using application level sharding Cobra uses consistent hashing to compute a shard/node id. The distribution of load is not very good on production data. # each \u220e represents a count of 15417. total 5003266 172.25.241.24:6379 [940378] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.239.218:6379 [879581] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.244.78:6379 [690303] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.86.253:6379 [627516] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.56.49:6379 [431098] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.28.138.193:6379 [343696] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.29.62.140:6379 [340555] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.145.223:6379 [277856] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.25.184.203:6379 [258128] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.31.104.188:6379 [214155] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e Mean Median Stddev Stddev/Mean 500326.6 387397.0 265549 0.530 The standard deviation is very high. Some nodes do way more work than others. Using redis-cluster, before resharding # each \u220e represents a count of 13921. total 5000873 172.26.42.94:6379 [849140] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.27.86.24:6379 [796878] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.27.36.226:6379 [759250] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.25.36:6379 [718014] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.34.11:6379 [409835] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.244.119:6379 [391286] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.25.145.138:6379 [386700] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.32.220:6379 [348391] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.25.225.42:6379 [341379] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e Mean Median Stddev Stddev/Mean 555652 409835 217322 0.391 The standard deviation decreased from 265500 to 217322, the ratio between the standard deviation and the mean is smaller. After the binpacking resharding # each \u220e represents a count of 10615. total 5004030 172.27.36.226:6379 [647515] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.25.36:6379 [581910] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.244.119:6379 [566576] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.27.86.24:6379 [562850] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.34.11:6379 [560954] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.25.225.42:6379 [551802] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.42.94:6379 [536849] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.25.145.138:6379 [499993] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.32.220:6379 [495581] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e Mean Median Stddev Stddev/Mean 556003 560954 45278 0.081 Visually we can see that the distribution is very well balanced. The Stddev/Mean ratio decreased by a factor of 6.5 (0.530 / 0.81).","title":"Resharding"},{"location":"resharding/#resharding","text":"","title":"Resharding"},{"location":"resharding/#using-rcc-to-reshard-a-redis-cluster-using-keyspace-notification","text":"rcc comes with 2 important tools, one for analyzing the keys access accross nodes, built on top of redis keyspace notifications (which in turns runs on top of redis PubSub). The instrumentation is done over a period of time to sample the key access patterns. A 'weights' file is created as part of this tool. That file is saved in the current working directory by default. It is a very simple csv file that show how often a key is accessed. $ head weights.csv _pubsub::foo,50 _pubsub::bar,53 _pubsub::baz,7 _pubsub::buz,19 _pubsub::blah,940 _pubsub::blooh,20 _pubsub::xxx,552 _pubsub::yyy,571 _pubsub::zzz,92 _pubsub::foo,5035 The second tool is used to reshard a cluster, and migrate slots to node using the bin-packing algorithm. To feed this algorithm, weights are required.","title":"Using rcc to reshard a redis cluster using keyspace notification"},{"location":"resharding/#generating-redis-cluster-traffic","text":"We use the cobra publish in batch mode command to send data to cobra , which internally send lots of XADD commands to our redis cluster. Cobra server started with: cobra run -r redis://localhost:11000 Cobra publishers: cobra publish --batch The redis cluster is started with: rcc make-cluster ; rcc has a convenience sub-command to generate config file for cluster (by default 3 masters and 3 replicas), and finally initialize the cluster.","title":"Generating redis cluster traffic."},{"location":"resharding/#keyspace-access-analysis-before-resharding","text":"rcc keyspace --redis_url redis://localhost:11000 --timeout 10 ... == Nodes == # each \u220e represents a count of 105. total 15515 127.0.0.1:11000 [ 6789] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 127.0.0.1:11002 [ 4983] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 127.0.0.1:11001 [ 3743] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e","title":"Keyspace access analysis before resharding"},{"location":"resharding/#resharding_1","text":"$ rcc reshard --redis_url redis://localhost:11000 file descriptors ulimit: 1024 resharding can be hungry, bump it with ulimit -n if needed == f3fa13802f339abb98ccb377e8a1a4eb957be987 / 127.0.0.1:11000 == migrated 0 slots Waiting for cluster view to be consistent... .== 8c67b776ab52ad756777866dcb8425cc866c71a3 / 127.0.0.1:11001 == migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrated 10 slots Waiting for cluster view to be consistent... ............== d50246e7e3914639759add181c71a2e3c879ed2f / 127.0.0.1:11002 == migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrating 1 keys migrated 11 slots Waiting for cluster view to be consistent... .....total migrated slots: 11 It roughtly looks like we took slots away from the first node, and gave them to the other two redis instances.","title":"Resharding"},{"location":"resharding/#keyspace-access-analysis-after-resharding","text":"rcc keyspace --redis_url redis://localhost:11000 --timeout 10 ... == Nodes == # each \u220e represents a count of 79. total 15114 127.0.0.1:11002 [ 5087] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 127.0.0.1:11000 [ 5040] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 127.0.0.1:11001 [ 4987] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e","title":"Keyspace access analysis after resharding"},{"location":"resharding/#larger-cobra-cluster","text":"The cluster has 10 masters and 10 replicas. We sample about 5,000,000 received commands (this takes around a minute). This use the new --count option of the keyspace command.","title":"Larger Cobra cluster"},{"location":"resharding/#using-application-level-sharding","text":"Cobra uses consistent hashing to compute a shard/node id. The distribution of load is not very good on production data. # each \u220e represents a count of 15417. total 5003266 172.25.241.24:6379 [940378] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.239.218:6379 [879581] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.244.78:6379 [690303] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.86.253:6379 [627516] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.56.49:6379 [431098] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.28.138.193:6379 [343696] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.29.62.140:6379 [340555] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.145.223:6379 [277856] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.25.184.203:6379 [258128] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.31.104.188:6379 [214155] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e Mean Median Stddev Stddev/Mean 500326.6 387397.0 265549 0.530 The standard deviation is very high. Some nodes do way more work than others.","title":"Using application level sharding"},{"location":"resharding/#using-redis-cluster-before-resharding","text":"# each \u220e represents a count of 13921. total 5000873 172.26.42.94:6379 [849140] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.27.86.24:6379 [796878] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.27.36.226:6379 [759250] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.25.36:6379 [718014] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.34.11:6379 [409835] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.244.119:6379 [391286] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.25.145.138:6379 [386700] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.32.220:6379 [348391] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.25.225.42:6379 [341379] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e Mean Median Stddev Stddev/Mean 555652 409835 217322 0.391 The standard deviation decreased from 265500 to 217322, the ratio between the standard deviation and the mean is smaller.","title":"Using redis-cluster, before resharding"},{"location":"resharding/#after-the-binpacking-resharding","text":"# each \u220e represents a count of 10615. total 5004030 172.27.36.226:6379 [647515] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.25.36:6379 [581910] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.244.119:6379 [566576] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.27.86.24:6379 [562850] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.24.34.11:6379 [560954] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.25.225.42:6379 [551802] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.42.94:6379 [536849] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.25.145.138:6379 [499993] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e 172.26.32.220:6379 [495581] \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e Mean Median Stddev Stddev/Mean 556003 560954 45278 0.081 Visually we can see that the distribution is very well balanced. The Stddev/Mean ratio decreased by a factor of 6.5 (0.530 / 0.81).","title":"After the binpacking resharding"}]}